Alright, imagine youâ€™re building your own digital city in the cloud. A VPC is like your walled estate in AWS â€” a virtual network where all your compute resources like EC2 instances live and communicate. Itâ€™s logically isolated from everyone elseâ€™s AWS networking, meaning you get full control over who can talk to whom. That isolation and control is a big deal in cloud networking.

In simple terms, itâ€™s a network you define within AWS â€” you choose the IP address ranges, subnets, routing rules, gateways, and all that networking jazz.

A VPC spans an entire AWS region and can contain subnets in multiple availability zones (AZs), giving you both flexibility and resilience


ğŸ§© Subnets â€” Dividing Your VPC

Now, think of your VPC like a big mansion. Inside that mansion, you have rooms â€” those are subnets. A subnet is just a range of IP addresses inside your VPC. You can launch EC2 instances or any other AWS resources inside those subnets.

Each subnet lives entirely within one availability zone (so they donâ€™t span across AZs), which is part of how AWS helps keep your infrastructure fault-tolerant.

But â€” and this is important â€” just creating a subnet doesnâ€™t automatically decide whether your servers can reach the internet or not. That depends on how you route traffic and what gateways you attach. Letâ€™s unpack that.

ğŸ›£ï¸ Route Tables â€” The Traffic Directors

Inside your VPC lives an implicit router that forwards traffic around based on rules you define in route tables. Each subnet is tied to a route table. If you donâ€™t assign one explicitly, AWS uses the main route table by default.

Think of route tables as the GPS instructions for your packets. They tell network traffic where to go â€” e.g., to the internet, to another subnet, or to a gateway. Each route has:

A destination â€” the IP range you want to get to.

A target â€” where you send the traffic.

So if you want your subnet to talk to the internet, you route 0.0.0.0/0 â€” AKA â€œeverything elseâ€ â€” to some kind of gateway.

Weâ€™ll come back to that part in a bit.

ğŸŒ Internet Gateway â€” The Front Door

An Internet Gateway (IGW) is like the front door of your mansion that lets traffic flow directly in and out to the public internet. If your subnet's route table has a route like:

Destination: 0.0.0.0/0  
Target: Internet Gateway


Then that subnet is considered a public subnet â€” meaning, as long as your instances have public IP addresses and security group rules, they can reach the outside world and the outside world can reach them too (if allowed).

This is where many engineers first panic â€” because suddenly your server can talk to everyone. But hold on! Later weâ€™ll talk about security groups which are like your mansionâ€™s bouncers. ğŸ•¶ï¸

ğŸ¤« Private Subnets and NAT Gateways â€” The Secret Back Rooms

Now letâ€™s talk about the part that sounds fancy but is actually very practical: private subnets.

Private subnets donâ€™t have direct access to the internet because their route tables do not send 0.0.0.0/0 traffic to the internet gateway. Now, that doesnâ€™t mean theyâ€™re stuck in the cave forever â€” you can enable outbound internet access for them by giving them a NAT Gateway.

NAT Gateway stands for Network Address Translation. It sits in a public subnet and acts like a masked go-between for private instances:

Instances in private subnets send internet traffic to a NAT Gateway.

The NAT Gateway translates their private IPs to its own public IP.

Traffic goes out to the internet, and responses come back â€” but outside systems canâ€™t initiate connections back to your instances.

So think of a NAT gateway like a secret back door attendant â€” allowing your internal servers to go out for updates and API calls, but stopping strangers from knocking on their doors. ğŸ•µï¸â€â™‚ï¸

ğŸ§  Public vs Private Subnets â€” Whatâ€™s the Difference?

Letâ€™s sum this up in plain English because AWS loves its buzzwords:

Public Subnets
ğŸ‘‰ Their route table sends 0.0.0.0/0 to an Internet Gateway
ğŸ‘‰ Instances here can have public IPs and communicate directly with the internet (if your security groups are polite enough)

Private Subnets
ğŸ‘‰ Their route table does NOT send 0.0.0.0/0 to an Internet Gateway
ğŸ‘‰ But they can route 0.0.0.0/0 to a NAT Gateway for outbound only access

So public vs private is really just about how the route table is wired up â€” not some magical AWS fairy dust. ğŸ§šâ€â™‚ï¸

ğŸ›¡ï¸ Quick Security Note

Before we wrap up, remember this: subnets and routing dictate where traffic can go, but security groups and network ACLs dictate what is allowed. Think of those as your bouncers and walls â€” they still decide who gets in or out even if the roads exist. Thatâ€™s a whole conversation of its own, but just keep in mind itâ€™s part of the bigger picture.

ğŸ Final Thoughts

So there you have it â€” a tour of AWS networking basics:

VPC = your private digital estate in the cloud.

Subnets = inside rooms, public or private.

Route tables = traffic GPS.

Internet Gateway = big front door to the world.

NAT Gateway = private back door access.

Networking can seem like wizardry at first, but once you break it down into pieces â€” itâ€™s really just controlling how data moves from point A to point B.

âš–ï¸ Load Balancers â€” The Traffic Managers You Canâ€™t Live Without

Alright, now that weâ€™ve talked about public and private subnets, this is the perfect time to talk about load balancers â€” because letâ€™s be honest, running one lonely EC2 instance in production is basically asking for trouble. ğŸ˜…

A load balancer does exactly what it sounds like: it takes incoming traffic and spreads it across multiple targets â€” like EC2 instances, containers, or even IP addresses â€” so no single thing gets overwhelmed.

Think of it like a very calm traffic cop at a busy intersection, redirecting cars so nobody crashes into each other.

In AWS, load balancers are part of a service called Elastic Load Balancing, or ELB â€” and there are three main types youâ€™ll see in the wild.

ğŸŒ Application Load Balancer (ALB)

Letâ€™s start with the most popular one these days: the Application Load Balancer, or ALB.

ALBs operate at Layer 7 of the OSI model â€” which basically means they understand HTTP and HTTPS traffic. Theyâ€™re smart. Likeâ€¦ annoyingly smart in a good way.

With an ALB, you can do things like:

Route traffic based on URL paths
/api goes here, /login goes there

Route based on hostnames
app.example.com vs admin.example.com

Support modern architectures like microservices and containers

This is why ALBs are the default choice for:

Web apps

REST APIs

Container-based workloads like ECS and EKS

From a networking point of view, ALBs usually sit in public subnets and forward traffic to targets living in private subnets â€” which is exactly how you keep your app exposed but your servers hidden. ğŸ”

âš¡ Network Load Balancer (NLB)

Next up: the Network Load Balancer, or NLB.

This one operates at Layer 4, meaning it doesnâ€™t care about URLs or headers â€” it just sees IP addresses and ports. Super fast. Very efficient. No small talk. ğŸï¸

NLBs are great when you need:

Ultra-low latency

High throughput

Static IP addresses

TCP, UDP, or TLS traffic

Youâ€™ll often see NLBs used for:

Databases

Real-time systems

Legacy applications

Or anything where speed matters more than fancy routing

Networking-wise, NLBs can also sit in public subnets or be internal-only, depending on your use case.

ğŸ›ï¸ Classic Load Balancer (CLB)

And then thereâ€™s the Classic Load Balancerâ€¦

Letâ€™s just say this politely:
It still exists.
AWS still supports it.
But you probably shouldnâ€™t start anything new with it. ğŸ˜¬

CLBs were the original load balancers before ALB and NLB existed. They can operate at both Layer 4 and Layer 7, but theyâ€™re missing a lot of modern features.

If you see one in production, itâ€™s usually because:

The app is old

Nobody wants to touch it

Or itâ€™s been running since the dawn of AWS time

For new workloads? ALB or NLB all the way.

ğŸ§  Where Load Balancers Fit in Your VPC

Now hereâ€™s where everything ties together.

A typical AWS networking setup looks something like this:

Internet Gateway attached to the VPC

Public subnets containing:

Load balancers

NAT Gateways

Private subnets containing:

EC2 instances

Containers

Databases

Traffic flow looks like this:

User hits the load balancer from the internet

Load balancer forwards traffic to private instances

Instances respond back through the load balancer

Private instances use NAT Gateway for outbound internet access

So the load balancer becomes your front desk, your shield, and your traffic manager â€” all in one.

ğŸ©º Health Checks â€” Because Trust Issues Are Healthy

One more thing load balancers do really well: health checks.

AWS load balancers constantly ask your targets:

â€œHey, you good?â€

If an instance doesnâ€™t respond correctly, the load balancer stops sending traffic to it automatically.

No alerts required.
No panic.
No midnight SSH sessions.

This is one of those features you donâ€™t appreciate until the day it saves your app from a full outage. ğŸ™Œ

ğŸ” Quick Recap â€” Networking + Load Balancers Together

Letâ€™s quickly zoom out and connect the dots:

VPC â€” your private network in AWS

Subnets â€” public and private slices of that network

Route tables â€” tell traffic where to go

Internet Gateway â€” inbound and outbound internet access

NAT Gateway â€” outbound-only internet for private subnets

Load Balancers â€” safely expose your app and spread traffic